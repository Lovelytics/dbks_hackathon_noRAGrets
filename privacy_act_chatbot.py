# Databricks notebook source
# MAGIC %pip install transformers==4.30.2 "unstructured[pdf,docx]==0.10.30" langchain==0.0.319 llama-index==0.9.3 databricks-vectorsearch==0.20 pydantic==1.10.9 mlflow==2.9.0 mypy_extensions==1.0.0 distro==1.9.0 pillow==10.2.0 filelock==3.11.0
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

catalog = "demo"
db = "hackathon"
volume_folder = f"/Volumes/{catalog}/{db}/privacy_act_docs/Colorado/"
df = (spark.readStream
        .format('cloudFiles')
        .option('cloudFiles.format', 'BINARYFILE')
        .option("pathGlobFilter", "*.pdf")
        .load('dbfs:'+volume_folder))

# Write the data as a Delta table
(df.writeStream
  .trigger(availableNow=True)
  .option("checkpointLocation", f'dbfs:{volume_folder}/checkpoints/raw_docs')
  .table(f'{catalog}.{db}.pdf_raw').awaitTermination())

# COMMAND ----------

from unstructured.partition.auto import partition
import re
import io

def extract_doc_text(x : bytes) -> str:
  # Read files and extract the values with unstructured
  sections = partition(file=io.BytesIO(x))
  def clean_section(txt):
    txt = re.sub(r'\n', '', txt)
    return re.sub(r' ?\.', '.', txt)
  # Default split is by section of document, concatenate them all together because we want to split by sentence instead.
  return "\n".join([clean_section(s.text) for s in sections]) 

# COMMAND ----------

import io
import re

with open("/Volumes/demo/hackathon/privacy_act_docs/Colorado/CPA-regulations.pdf", "rb") as fh:
    bytes_stream = bytes(fh.read())
doc = extract_doc_text(bytes_stream)
print(doc)

# COMMAND ----------

from llama_index.langchain_helpers.text_splitter import SentenceSplitter
from llama_index import Document, set_global_tokenizer
from transformers import AutoTokenizer
from pyspark.sql.functions import pandas_udf
from typing import Iterator
import pandas as pd
import os

# Reduce the arrow batch size as our PDF can be big in memory
spark.conf.set("spark.sql.execution.arrow.maxRecordsPerBatch", 10)

os.environ["HF_HOME"] = '/tmp'

@pandas_udf("array<string>")
def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:
    #set llama2 as tokenizer to match our model size (will stay below BGE 1024 limit)
    set_global_tokenizer(
      AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer", cache_dir = '/tmp')
    )
    #Sentence splitter from llama_index to split on sentences
    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)
    def extract_and_split(b):
      txt = extract_doc_text(b)
      nodes = splitter.get_nodes_from_documents([Document(text=txt)])
      
    my_list = []
    for x in batch_iter:
      my_list.append(x.apply(extract_and_split))
      print(my_list)
      #  yield x.apply(extract_and_split)

    return my_list

# COMMAND ----------

from mlflow.deployments import get_deploy_client
from pprint import pprint

# bge-large-en Foundation models are available using the /serving-endpoints/databricks-bge-large-en/invocations api. 
deploy_client = get_deploy_client("databricks")

embeddings = deploy_client.predict(endpoint="databricks-bge-large-en", inputs={"input": ["What is Apache Spark?"]})
pprint(embeddings)

# COMMAND ----------

# MAGIC %sql
# MAGIC --Note that we need to enable Change Data Feed on the table to create the index
# MAGIC CREATE TABLE IF NOT EXISTS demo.hackathon.databricks_pdf_documentation (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   url STRING,
# MAGIC   content STRING,
# MAGIC   embedding ARRAY <FLOAT>
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true); 

# COMMAND ----------

@pandas_udf("array<float>")
def get_embedding(contents: pd.Series) -> pd.Series:
    import mlflow.deployments
    deploy_client = mlflow.deployments.get_deploy_client("databricks")
    def get_embeddings(batch):
        #Note: this will fail if an exception is thrown during embedding creation (add try/except if needed) 
        response = deploy_client.predict(endpoint="databricks-bge-large-en", inputs={"input": batch})
        return [e['embedding'] for e in response.data]

    # Splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.
    max_batch_size = 150
    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]

    # Process each batch and collect the results
    all_embeddings = []
    for batch in batches:
        all_embeddings += get_embeddings(batch.tolist())

    return pd.Series(all_embeddings)

# COMMAND ----------



# COMMAND ----------

from pyspark.sql import functions as F
import mypy_extensions
from llama_index.langchain_helpers.text_splitter import SentenceSplitter
from llama_index import Document, set_global_tokenizer
from transformers import AutoTokenizer
from pyspark.sql.functions import pandas_udf
from typing import Iterator
import pandas as pd
import os

# # Reduce the arrow batch size as our PDF can be big in memory
# spark.conf.set("spark.sql.execution.arrow.maxRecordsPerBatch", 10)

# os.environ["HF_HOME"] = '/tmp'

# set_global_tokenizer(
#   AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer", cache_dir = '/tmp')
# )
# #Sentence splitter from llama_index to split on sentences
# splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)
# def extract_and_split(b):
#   txt = extract_doc_text(b)
#   nodes = splitter.get_nodes_from_documents([Document(text=txt)])
#   return [n.text for n in nodes]

# col = spark.table('demo.hackathon.pdf_raw')
# pd_col = col.toPandas()

# pd_col["content"].apply(extract_and_split)

volume_folder = f"/Volumes/demo/hackathon/privacy_act_docs/Colorado"

(spark.readStream.table('demo.hackathon.pdf_raw')
      .withColumn("content", F.explode(read_as_chunk("content")))
      .withColumn("embedding", get_embedding("content"))
      .selectExpr('path as url', 'content', 'embedding')
  .writeStream
    .trigger(availableNow=True)
    .option("checkpointLocation", f'dbfs:{volume_folder}/checkpoints/pdf_chunk')
    .table('demo.hackathon.databricks_pdf_documentation').awaitTermination())

# (spark.readStream.table('demo.hackathon.pdf_raw')
#       .withColumn("content", F.explode(read_as_chunk("content")))
#       .withColumn("embedding", get_embedding("content"))
#       .selectExpr('path as url', 'content', 'embedding')
#   .writeStream
#     .trigger(availableNow=True)
#     .option("checkpointLocation", f'dbfs:/Volumes/demo/hackathon/privacy_act_docs/Colorado/checkpoints/pdf_chunk')
#     .table('demo.hackathon.databricks_pdf_documentation').awaitTermination())

# #Let's also add our documentation web page from the simple demo (make sure you run the quickstart demo first)
# if table_exists('demo.hackathon.databricks_documentation'):
#   (spark.readStream.table('databricks_documentation')
#       .withColumn('embedding', get_embedding("content"))
#       .select('url', 'content', 'embedding')
#   .writeStream
#     .trigger(availableNow=True)
#     .option("checkpointLocation", f'dbfs:{volume_folder}/checkpoints/docs_chunks')
#     .table('databricks_pdf_documentation').awaitTermination())

# COMMAND ----------


