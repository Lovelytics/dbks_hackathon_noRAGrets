{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efcc8d2e-db0b-4e8b-a16e-61d8926e22b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers==4.30.2 \"unstructured[pdf,docx]==0.10.30\" llama-index==0.9.40 databricks-vectorsearch==0.20 pydantic==1.10.9 mlflow==2.9.0 protobuf==3.20.0 openai==1.10.0 langchain-openai langchain torch torchvision torchaudio FlagEmbedding\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6dcb090-626b-4bf2-9ec9-c12b4007e9f1",
     "showTitle": true,
     "title": "Helper function for read_from_text"
    }
   },
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "import re\n",
    "import io\n",
    "\n",
    "def extract_doc_text(x : bytes) -> str:\n",
    "  # Read files and extract the values with unstructured\n",
    "  sections = partition(file=io.BytesIO(x))\n",
    "  print(sections)\n",
    "  def clean_section(txt):\n",
    "    txt = re.sub(r'\\n', '', txt)\n",
    "    return re.sub(r' ?\\.', '.', txt)\n",
    "  # Default split is by section of document, concatenate them all together because we want to split by sentence instead.\n",
    "  return \"\\n\".join([clean_section(s.text) for s in sections]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343e4e39-fbb2-496f-8562-eb410a4980d1",
     "showTitle": true,
     "title": "OpenAI Client and read as chunk function defined"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.langchain_helpers.text_splitter import SentenceSplitter\n",
    "from llama_index.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index import Document, set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from pyspark.sql import functions as F\n",
    "import mypy_extensions\n",
    "from openai import AzureOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = dbutils.secrets.get(scope='dev_demo', key='azure_openai_api_key')\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://nous-ue2-openai-sbx-openai.openai.azure.com/\"\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"nous-ue2-openai-sbx-base-deploy-text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-05-15\",\n",
    ")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key = dbutils.secrets.get(scope='dev_demo', key='azure_openai_api_key'),\n",
    "    api_version = \"2023-05-15\",\n",
    "    azure_endpoint = \"https://nous-ue2-openai-sbx-openai.openai.azure.com/\",\n",
    "    )\n",
    "\n",
    "# Reduce the arrow batch size as our PDF can be big in memory\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = '/tmp'\n",
    "\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    #set embedding model\n",
    "    # embed_model = \"nous-ue2-openai-sbx-base-deploy-text-embedding-ada-002\"\n",
    "    #set llama2 as tokenizer to match our model size (will stay below BGE 1024 limit)\n",
    "    set_global_tokenizer(\n",
    "      AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\", cache_dir = '/tmp')\n",
    "    )\n",
    "    # splitter = SemanticSplitterNodeParser(\n",
    "    # buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embeddings\n",
    "    # )\n",
    "    #Sentence splitter from llama_index to split on sentences\n",
    "    base_splitter = SentenceSplitter(chunk_size=500, chunk_overlap=25)\n",
    "    def extract_and_split(b):\n",
    "      txt = extract_doc_text(b)\n",
    "      nodes = base_splitter.get_nodes_from_documents([Document(text=txt)])\n",
    "      logging.info(f\"from chunk function: {txt}\")\n",
    "      \n",
    "      return [n.text for n in nodes]\n",
    "\n",
    "    for x in batch_iter:\n",
    "        yield x.apply(extract_and_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "407179c8-07b0-47af-846a-e2a56da65afe",
     "showTitle": true,
     "title": "No need to run this (table already created)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Note that we need to enable Change Data Feed on the table to create the index\n",
    "CREATE TABLE IF NOT EXISTS demo.hackathon.databricks_pdf_documentation_openai (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING,\n",
    "  embedding ARRAY <FLOAT>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1209e2c5-81fd-41e9-8dcc-ca1435297c97",
     "showTitle": true,
     "title": "Get Embeddings function defined"
    }
   },
   "outputs": [],
   "source": [
    "def open_ai_embeddings(contents):\n",
    "    embed_model = \"nous-ue2-openai-sbx-base-deploy-text-embedding-ada-002\"\n",
    "\n",
    "    response = client.embeddings.create(\n",
    "        input = contents,\n",
    "        model = embed_model\n",
    "    )\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517a7c68-f552-4571-89df-f050514aea2c",
     "showTitle": true,
     "title": "Write to databricks_pdf_documentation_openai"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import mypy_extensions\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# # Reduce the arrow batch size as our PDF can be big in memory\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "\n",
    "# os.environ[\"HF_HOME\"] = '/tmp'\n",
    "\n",
    "volume_folder = f\"/Volumes/demo/hackathon/privacy_act_docs/*\"\n",
    "\n",
    "temp = (spark.table('demo.hackathon.pdf_raw')\n",
    "        .withColumn(\"content\", F.explode(read_as_chunk(\"content\")))\n",
    "        .withColumn(\"embedding\", F.lit(open_ai_embeddings(\"content\")))\n",
    "        .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "        .withColumn(\"state\", F.split(F.col(\"path\"), \"/\")[5])\n",
    "        .selectExpr('id', 'path as url', 'content', 'embedding', 'state')\n",
    "        )\n",
    "\n",
    "(temp.write\n",
    "    .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/pdf_chunk_openai')\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable('demo.hackathon.databricks_pdf_documentation_openai'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54e1d127-cc42-4776-8193-9bfff927f9aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# workaround for not having ML cluster\n",
    "temp = (spark.table('demo.hackathon.databricks_pdf_documentation_openai')\n",
    "        .withColumn(\"state\", F.split(F.col(\"url\"), \"/\")[5])\n",
    "        .selectExpr('id', 'url', 'content', 'embedding', 'state')\n",
    "        )\n",
    "\n",
    "(temp.write\n",
    "    .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/pdf_chunk_openai')\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable('demo.hackathon.databricks_pdf_documentation_openai'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ef43ca-9e3c-42ae-a5b3-f9491e3d7742",
     "showTitle": true,
     "title": "Definitions for vector search"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()\n",
    "vs_index_fullname = \"demo.hackathon.openai_self_managed_index_v3\"\n",
    "endpoint_name = \"openai_vector_search_v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1d1a845-fc48-4ab8-93fb-ba86622cb144",
     "showTitle": true,
     "title": "No need to run again (already created)"
    }
   },
   "outputs": [],
   "source": [
    "vsc.create_endpoint(name=endpoint_name, endpoint_type=\"STANDARD\")\n",
    "vsc.create_delta_sync_index(\n",
    "    endpoint_name=endpoint_name,\n",
    "    index_name=vs_index_fullname,\n",
    "    source_table_name=\"demo.hackathon.databricks_pdf_documentation_openai\",\n",
    "    pipeline_type=\"TRIGGERED\", #Sync needs to be manually triggered\n",
    "    primary_key=\"id\",\n",
    "    embedding_dimension=1536, #Match your model embedding size (bge)\n",
    "    embedding_vector_column=\"embedding\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8317db9b-387e-40ee-a1d3-22048bdfb62b",
     "showTitle": true,
     "title": "Run this to resync our index table with new results"
    }
   },
   "outputs": [],
   "source": [
    "# Resync our index with new data\n",
    "vsc.get_index(endpoint_name, vs_index_fullname).sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f3edab-0d53-4804-b706-cf6e4f1e3d2c",
     "showTitle": true,
     "title": "Test prompts (call embedding endpoint here)"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "from pprint import pprint\n",
    "# bge-large-en Foundation models are available using the /serving-endpoints/databricks-bge-large-en/invocations api. \n",
    "deploy_client = get_deploy_client(\"databricks\")\n",
    "query = f\"When does the Colorado Privacy Act take effect?\"\n",
    "\n",
    "results = vsc.get_index(endpoint_name, vs_index_fullname).similarity_search(\n",
    "  query_vector = open_ai_embeddings(query),\n",
    "  columns=[\"state\", \"url\", \"content\"],\n",
    "  num_results=10)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "423e1a31-2b1a-40f6-832e-3806677c8af0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This filter is *working* hard-coded, but the RAG is still coming back with incorrect results.\n",
    "\n",
    "# from mlflow.deployments import get_deploy_client\n",
    "# from pprint import pprint\n",
    "# # bge-large-en Foundation models are available using the /serving-endpoints/databricks-bge-large-en/invocations api. \n",
    "# deploy_client = get_deploy_client(\"databricks\")\n",
    "# query = f\"When does the Colorado Privacy Act take effect?\"\n",
    "# #content\n",
    "# results = vsc.get_index(endpoint_name, vs_index_fullname).similarity_search(\n",
    "#   query_vector=open_ai_embeddings(query),\n",
    "#   columns=[\"state\", \"url\", \"content\"],\n",
    "#   filters={\"state\": \"Colorado\"},\n",
    "#   num_results=10)\n",
    "# docs = results.get('result', {}).get('data_array', [])\n",
    "# pprint(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fb3803b-8637-4edf-af41-01e4f0b454bc",
     "showTitle": true,
     "title": "Reranking with bge-reranker-large"
    }
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-large\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-large\")\n",
    "\n",
    "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "query_and_docs = [[query, d[1]] for d in docs]\n",
    "\n",
    "scores = reranker.compute_score(query_and_docs)\n",
    "\n",
    "reranked_docs = sorted(list(zip(docs, scores)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pprint(reranked_docs)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "privacy_act_chatbot_clean_cg",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
